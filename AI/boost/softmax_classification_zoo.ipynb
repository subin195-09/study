{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "1.18.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "\n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter = ',', dtype = np.float32)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, -1]\n",
    "\n",
    "nb_classes = 7 #0 ~ 6\n",
    "\n",
    "#onehot 형태로 만들기\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight, bias setting\n",
    "\n",
    "W = tf.Variable(tf.random.normal((16, nb_classes)), name = 'weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name = 'bias')\n",
    "variables = [W, b]\n",
    "\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, from_logits=True)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "\n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 5.610101222991943, Acc: 0.1683168262243271\n",
      "Steps: 2 Loss: 5.005418300628662, Acc: 0.1683168262243271\n",
      "Steps: 3 Loss: 4.456991195678711, Acc: 0.1683168262243271\n",
      "Steps: 4 Loss: 3.9935643672943115, Acc: 0.1683168262243271\n",
      "Steps: 5 Loss: 3.654680013656616, Acc: 0.3564356565475464\n",
      "Steps: 6 Loss: 3.4425995349884033, Acc: 0.3663366436958313\n",
      "Steps: 7 Loss: 3.309016704559326, Acc: 0.41584157943725586\n",
      "Steps: 8 Loss: 3.210127353668213, Acc: 0.3861386179924011\n",
      "Steps: 9 Loss: 3.124446392059326, Acc: 0.39603960514068604\n",
      "Steps: 10 Loss: 3.0438120365142822, Acc: 0.39603960514068604\n",
      "Steps: 11 Loss: 2.965470790863037, Acc: 0.39603960514068604\n",
      "Steps: 12 Loss: 2.888542890548706, Acc: 0.4455445408821106\n",
      "Steps: 13 Loss: 2.812761068344116, Acc: 0.4455445408821106\n",
      "Steps: 14 Loss: 2.7380566596984863, Acc: 0.4455445408821106\n",
      "Steps: 15 Loss: 2.6644275188446045, Acc: 0.48514851927757263\n",
      "Steps: 16 Loss: 2.5918962955474854, Acc: 0.48514851927757263\n",
      "Steps: 17 Loss: 2.5204966068267822, Acc: 0.49504950642585754\n",
      "Steps: 18 Loss: 2.450270652770996, Acc: 0.48514851927757263\n",
      "Steps: 19 Loss: 2.3812668323516846, Acc: 0.49504950642585754\n",
      "Steps: 20 Loss: 2.3135392665863037, Acc: 0.49504950642585754\n",
      "Steps: 21 Loss: 2.2471470832824707, Acc: 0.49504950642585754\n",
      "Steps: 22 Loss: 2.1821563243865967, Acc: 0.49504950642585754\n",
      "Steps: 23 Loss: 2.118638515472412, Acc: 0.49504950642585754\n",
      "Steps: 24 Loss: 2.056671380996704, Acc: 0.5148515105247498\n",
      "Steps: 25 Loss: 1.996336579322815, Acc: 0.5247524976730347\n",
      "Steps: 26 Loss: 1.9377213716506958, Acc: 0.5247524976730347\n",
      "Steps: 27 Loss: 1.8809136152267456, Acc: 0.5247524976730347\n",
      "Steps: 28 Loss: 1.826001763343811, Acc: 0.5346534848213196\n",
      "Steps: 29 Loss: 1.773068904876709, Acc: 0.5247524976730347\n",
      "Steps: 30 Loss: 1.7221876382827759, Acc: 0.5445544719696045\n",
      "Steps: 31 Loss: 1.6734132766723633, Acc: 0.5445544719696045\n",
      "Steps: 32 Loss: 1.6267778873443604, Acc: 0.5445544719696045\n",
      "Steps: 33 Loss: 1.5822843313217163, Acc: 0.5643564462661743\n",
      "Steps: 34 Loss: 1.5399030447006226, Acc: 0.5643564462661743\n",
      "Steps: 35 Loss: 1.4995723962783813, Acc: 0.5643564462661743\n",
      "Steps: 36 Loss: 1.4612046480178833, Acc: 0.5643564462661743\n",
      "Steps: 37 Loss: 1.424689769744873, Acc: 0.5544554591178894\n",
      "Steps: 38 Loss: 1.3899067640304565, Acc: 0.5445544719696045\n",
      "Steps: 39 Loss: 1.3567299842834473, Acc: 0.5247524976730347\n",
      "Steps: 40 Loss: 1.3250372409820557, Acc: 0.5247524976730347\n",
      "Steps: 41 Loss: 1.2947139739990234, Acc: 0.5247524976730347\n",
      "Steps: 42 Loss: 1.2656574249267578, Acc: 0.5247524976730347\n",
      "Steps: 43 Loss: 1.2377772331237793, Acc: 0.5247524976730347\n",
      "Steps: 44 Loss: 1.2109965085983276, Acc: 0.5247524976730347\n",
      "Steps: 45 Loss: 1.1852500438690186, Acc: 0.5247524976730347\n",
      "Steps: 46 Loss: 1.1604838371276855, Acc: 0.5247524976730347\n",
      "Steps: 47 Loss: 1.1366524696350098, Acc: 0.5247524976730347\n",
      "Steps: 48 Loss: 1.1137170791625977, Acc: 0.5445544719696045\n",
      "Steps: 49 Loss: 1.0916454792022705, Acc: 0.5445544719696045\n",
      "Steps: 50 Loss: 1.0704081058502197, Acc: 0.6138613820075989\n",
      "Steps: 51 Loss: 1.0499790906906128, Acc: 0.6138613820075989\n",
      "Steps: 52 Loss: 1.0303336381912231, Acc: 0.6138613820075989\n",
      "Steps: 53 Loss: 1.0114483833312988, Acc: 0.6237623691558838\n",
      "Steps: 54 Loss: 0.9933003187179565, Acc: 0.6336633563041687\n",
      "Steps: 55 Loss: 0.9758667349815369, Acc: 0.6435643434524536\n",
      "Steps: 56 Loss: 0.9591245055198669, Acc: 0.6831682920455933\n",
      "Steps: 57 Loss: 0.9430503249168396, Acc: 0.7029703259468079\n",
      "Steps: 58 Loss: 0.9276208281517029, Acc: 0.7029703259468079\n",
      "Steps: 59 Loss: 0.9128122329711914, Acc: 0.7029703259468079\n",
      "Steps: 60 Loss: 0.8986005187034607, Acc: 0.7128713130950928\n",
      "Steps: 61 Loss: 0.8849614858627319, Acc: 0.7128713130950928\n",
      "Steps: 62 Loss: 0.8718711137771606, Acc: 0.7128713130950928\n",
      "Steps: 63 Loss: 0.8593052625656128, Acc: 0.7128713130950928\n",
      "Steps: 64 Loss: 0.8472403883934021, Acc: 0.7128713130950928\n",
      "Steps: 65 Loss: 0.8356529474258423, Acc: 0.7128713130950928\n",
      "Steps: 66 Loss: 0.8245200514793396, Acc: 0.7227723002433777\n",
      "Steps: 67 Loss: 0.8138192892074585, Acc: 0.7524752616882324\n",
      "Steps: 68 Loss: 0.8035292029380798, Acc: 0.7524752616882324\n",
      "Steps: 69 Loss: 0.7936285734176636, Acc: 0.7821782231330872\n",
      "Steps: 70 Loss: 0.7840972542762756, Acc: 0.7920792102813721\n",
      "Steps: 71 Loss: 0.7749159336090088, Acc: 0.7920792102813721\n",
      "Steps: 72 Loss: 0.766066312789917, Acc: 0.801980197429657\n",
      "Steps: 73 Loss: 0.7575304508209229, Acc: 0.801980197429657\n",
      "Steps: 74 Loss: 0.7492917776107788, Acc: 0.801980197429657\n",
      "Steps: 75 Loss: 0.7413343191146851, Acc: 0.801980197429657\n",
      "Steps: 76 Loss: 0.733643114566803, Acc: 0.8118811845779419\n",
      "Steps: 77 Loss: 0.7262038588523865, Acc: 0.8118811845779419\n",
      "Steps: 78 Loss: 0.7190031409263611, Acc: 0.8118811845779419\n",
      "Steps: 79 Loss: 0.712028443813324, Acc: 0.8118811845779419\n",
      "Steps: 80 Loss: 0.7052679061889648, Acc: 0.8118811845779419\n",
      "Steps: 81 Loss: 0.6987101435661316, Acc: 0.8118811845779419\n",
      "Steps: 82 Loss: 0.6923449635505676, Acc: 0.8118811845779419\n",
      "Steps: 83 Loss: 0.6861622929573059, Acc: 0.8118811845779419\n",
      "Steps: 84 Loss: 0.6801532506942749, Acc: 0.8118811845779419\n",
      "Steps: 85 Loss: 0.6743088364601135, Acc: 0.8118811845779419\n",
      "Steps: 86 Loss: 0.6686211228370667, Acc: 0.8118811845779419\n",
      "Steps: 87 Loss: 0.663082480430603, Acc: 0.8118811845779419\n",
      "Steps: 88 Loss: 0.6576856970787048, Acc: 0.8118811845779419\n",
      "Steps: 89 Loss: 0.6524242162704468, Acc: 0.8118811845779419\n",
      "Steps: 90 Loss: 0.6472917199134827, Acc: 0.8217821717262268\n",
      "Steps: 91 Loss: 0.6422823071479797, Acc: 0.8217821717262268\n",
      "Steps: 92 Loss: 0.6373906135559082, Acc: 0.8217821717262268\n",
      "Steps: 93 Loss: 0.6326112151145935, Acc: 0.8217821717262268\n",
      "Steps: 94 Loss: 0.6279395222663879, Acc: 0.8217821717262268\n",
      "Steps: 95 Loss: 0.6233707666397095, Acc: 0.8217821717262268\n",
      "Steps: 96 Loss: 0.6189007759094238, Acc: 0.8316831588745117\n",
      "Steps: 97 Loss: 0.6145254969596863, Acc: 0.8316831588745117\n",
      "Steps: 98 Loss: 0.6102409362792969, Acc: 0.8415841460227966\n",
      "Steps: 99 Loss: 0.606043815612793, Acc: 0.8415841460227966\n",
      "Steps: 100 Loss: 0.6019306182861328, Acc: 0.8415841460227966\n",
      "Steps: 101 Loss: 0.5978981256484985, Acc: 0.8712871074676514\n",
      "Steps: 102 Loss: 0.5939433574676514, Acc: 0.8712871074676514\n",
      "Steps: 103 Loss: 0.5900635123252869, Acc: 0.8712871074676514\n",
      "Steps: 104 Loss: 0.5862558484077454, Acc: 0.8712871074676514\n",
      "Steps: 105 Loss: 0.5825178623199463, Acc: 0.8712871074676514\n",
      "Steps: 106 Loss: 0.5788471102714539, Acc: 0.8712871074676514\n",
      "Steps: 107 Loss: 0.5752413272857666, Acc: 0.8712871074676514\n",
      "Steps: 108 Loss: 0.5716983079910278, Acc: 0.8712871074676514\n",
      "Steps: 109 Loss: 0.56821608543396, Acc: 0.8712871074676514\n",
      "Steps: 110 Loss: 0.5647926330566406, Acc: 0.8712871074676514\n",
      "Steps: 111 Loss: 0.561426043510437, Acc: 0.8712871074676514\n",
      "Steps: 112 Loss: 0.5581147074699402, Acc: 0.8712871074676514\n",
      "Steps: 113 Loss: 0.5548567175865173, Acc: 0.8712871074676514\n",
      "Steps: 114 Loss: 0.5516506433486938, Acc: 0.8712871074676514\n",
      "Steps: 115 Loss: 0.5484948754310608, Acc: 0.8712871074676514\n",
      "Steps: 116 Loss: 0.5453879833221436, Acc: 0.8712871074676514\n",
      "Steps: 117 Loss: 0.5423285365104675, Acc: 0.8712871074676514\n",
      "Steps: 118 Loss: 0.5393152236938477, Acc: 0.8712871074676514\n",
      "Steps: 119 Loss: 0.5363466739654541, Acc: 0.8712871074676514\n",
      "Steps: 120 Loss: 0.5334218144416809, Acc: 0.8712871074676514\n",
      "Steps: 121 Loss: 0.5305393934249878, Acc: 0.8712871074676514\n",
      "Steps: 122 Loss: 0.5276981592178345, Acc: 0.8712871074676514\n",
      "Steps: 123 Loss: 0.5248972177505493, Acc: 0.8712871074676514\n",
      "Steps: 124 Loss: 0.5221354365348816, Acc: 0.8712871074676514\n",
      "Steps: 125 Loss: 0.5194118022918701, Acc: 0.8712871074676514\n",
      "Steps: 126 Loss: 0.516725480556488, Acc: 0.8712871074676514\n",
      "Steps: 127 Loss: 0.5140753388404846, Acc: 0.8712871074676514\n",
      "Steps: 128 Loss: 0.5114606618881226, Acc: 0.8712871074676514\n",
      "Steps: 129 Loss: 0.5088804364204407, Acc: 0.8712871074676514\n",
      "Steps: 130 Loss: 0.506334125995636, Acc: 0.8712871074676514\n",
      "Steps: 131 Loss: 0.5038204789161682, Acc: 0.8712871074676514\n",
      "Steps: 132 Loss: 0.5013390183448792, Acc: 0.8712871074676514\n",
      "Steps: 133 Loss: 0.49888908863067627, Acc: 0.8712871074676514\n",
      "Steps: 134 Loss: 0.49646979570388794, Acc: 0.8712871074676514\n",
      "Steps: 135 Loss: 0.49408042430877686, Acc: 0.8712871074676514\n",
      "Steps: 136 Loss: 0.49172043800354004, Acc: 0.8712871074676514\n",
      "Steps: 137 Loss: 0.48938921093940735, Acc: 0.8712871074676514\n",
      "Steps: 138 Loss: 0.48708587884902954, Acc: 0.8712871074676514\n",
      "Steps: 139 Loss: 0.48481011390686035, Acc: 0.8712871074676514\n",
      "Steps: 140 Loss: 0.4825611412525177, Acc: 0.8712871074676514\n",
      "Steps: 141 Loss: 0.4803384840488434, Acc: 0.8712871074676514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 142 Loss: 0.47814154624938965, Acc: 0.8712871074676514\n",
      "Steps: 143 Loss: 0.47596976161003113, Acc: 0.8712871074676514\n",
      "Steps: 144 Loss: 0.4738227128982544, Acc: 0.8712871074676514\n",
      "Steps: 145 Loss: 0.47169986367225647, Acc: 0.8712871074676514\n",
      "Steps: 146 Loss: 0.469600647687912, Acc: 0.8811880946159363\n",
      "Steps: 147 Loss: 0.4675246775150299, Acc: 0.8811880946159363\n",
      "Steps: 148 Loss: 0.4654715061187744, Acc: 0.8811880946159363\n",
      "Steps: 149 Loss: 0.46344059705734253, Acc: 0.8811880946159363\n",
      "Steps: 150 Loss: 0.4614315330982208, Acc: 0.8811880946159363\n",
      "Steps: 151 Loss: 0.4594440162181854, Acc: 0.8811880946159363\n",
      "Steps: 152 Loss: 0.4574774503707886, Acc: 0.8811880946159363\n",
      "Steps: 153 Loss: 0.4555315375328064, Acc: 0.8811880946159363\n",
      "Steps: 154 Loss: 0.4536059498786926, Acc: 0.8811880946159363\n",
      "Steps: 155 Loss: 0.45170027017593384, Acc: 0.8811880946159363\n",
      "Steps: 156 Loss: 0.44981399178504944, Acc: 0.8811880946159363\n",
      "Steps: 157 Loss: 0.44794702529907227, Acc: 0.8811880946159363\n",
      "Steps: 158 Loss: 0.4460987448692322, Acc: 0.8811880946159363\n",
      "Steps: 159 Loss: 0.44426894187927246, Acc: 0.8811880946159363\n",
      "Steps: 160 Loss: 0.44245728850364685, Acc: 0.8811880946159363\n",
      "Steps: 161 Loss: 0.44066348671913147, Acc: 0.8811880946159363\n",
      "Steps: 162 Loss: 0.4388871192932129, Acc: 0.8811880946159363\n",
      "Steps: 163 Loss: 0.4371280372142792, Acc: 0.8811880946159363\n",
      "Steps: 164 Loss: 0.4353858232498169, Acc: 0.8811880946159363\n",
      "Steps: 165 Loss: 0.4336601793766022, Acc: 0.8811880946159363\n",
      "Steps: 166 Loss: 0.43195098638534546, Acc: 0.8910890817642212\n",
      "Steps: 167 Loss: 0.4302576780319214, Acc: 0.8910890817642212\n",
      "Steps: 168 Loss: 0.42858028411865234, Acc: 0.8910890817642212\n",
      "Steps: 169 Loss: 0.4269183278083801, Acc: 0.8910890817642212\n",
      "Steps: 170 Loss: 0.4252716898918152, Acc: 0.8910890817642212\n",
      "Steps: 171 Loss: 0.42364004254341125, Acc: 0.8910890817642212\n",
      "Steps: 172 Loss: 0.42202311754226685, Acc: 0.8910890817642212\n",
      "Steps: 173 Loss: 0.4204207956790924, Acc: 0.8910890817642212\n",
      "Steps: 174 Loss: 0.4188327193260193, Acc: 0.8910890817642212\n",
      "Steps: 175 Loss: 0.41725873947143555, Acc: 0.8910890817642212\n",
      "Steps: 176 Loss: 0.4156986176967621, Acc: 0.8910890817642212\n",
      "Steps: 177 Loss: 0.4141520857810974, Acc: 0.8910890817642212\n",
      "Steps: 178 Loss: 0.4126189947128296, Acc: 0.8910890817642212\n",
      "Steps: 179 Loss: 0.41109907627105713, Acc: 0.8910890817642212\n",
      "Steps: 180 Loss: 0.4095922112464905, Acc: 0.8910890817642212\n",
      "Steps: 181 Loss: 0.40809816122055054, Acc: 0.8910890817642212\n",
      "Steps: 182 Loss: 0.4066166579723358, Acc: 0.8910890817642212\n",
      "Steps: 183 Loss: 0.40514764189720154, Acc: 0.8910890817642212\n",
      "Steps: 184 Loss: 0.40369081497192383, Acc: 0.8910890817642212\n",
      "Steps: 185 Loss: 0.40224602818489075, Acc: 0.8910890817642212\n",
      "Steps: 186 Loss: 0.40081316232681274, Acc: 0.8910890817642212\n",
      "Steps: 187 Loss: 0.3993920087814331, Acc: 0.8910890817642212\n",
      "Steps: 188 Loss: 0.3979823887348175, Acc: 0.8910890817642212\n",
      "Steps: 189 Loss: 0.39658409357070923, Acc: 0.8910890817642212\n",
      "Steps: 190 Loss: 0.39519697427749634, Acc: 0.8910890817642212\n",
      "Steps: 191 Loss: 0.39382097125053406, Acc: 0.8910890817642212\n",
      "Steps: 192 Loss: 0.3924557864665985, Acc: 0.8910890817642212\n",
      "Steps: 193 Loss: 0.3911014199256897, Acc: 0.8910890817642212\n",
      "Steps: 194 Loss: 0.38975754380226135, Acc: 0.8910890817642212\n",
      "Steps: 195 Loss: 0.3884241580963135, Acc: 0.9108911156654358\n",
      "Steps: 196 Loss: 0.38710102438926697, Acc: 0.9108911156654358\n",
      "Steps: 197 Loss: 0.38578811287879944, Acc: 0.9108911156654358\n",
      "Steps: 198 Loss: 0.384485125541687, Acc: 0.9108911156654358\n",
      "Steps: 199 Loss: 0.3831920921802521, Acc: 0.9108911156654358\n",
      "Steps: 200 Loss: 0.38190871477127075, Acc: 0.9108911156654358\n",
      "Steps: 201 Loss: 0.38063502311706543, Acc: 0.9108911156654358\n",
      "Steps: 202 Loss: 0.379370778799057, Acc: 0.9108911156654358\n",
      "Steps: 203 Loss: 0.3781159520149231, Acc: 0.9108911156654358\n",
      "Steps: 204 Loss: 0.37687042355537415, Acc: 0.9108911156654358\n",
      "Steps: 205 Loss: 0.3756338357925415, Acc: 0.9108911156654358\n",
      "Steps: 206 Loss: 0.3744063377380371, Acc: 0.9108911156654358\n",
      "Steps: 207 Loss: 0.37318775057792664, Acc: 0.9108911156654358\n",
      "Steps: 208 Loss: 0.37197792530059814, Acc: 0.9108911156654358\n",
      "Steps: 209 Loss: 0.3707767128944397, Acc: 0.9108911156654358\n",
      "Steps: 210 Loss: 0.3695841133594513, Acc: 0.9108911156654358\n",
      "Steps: 211 Loss: 0.36839988827705383, Acc: 0.9108911156654358\n",
      "Steps: 212 Loss: 0.3672240674495697, Acc: 0.9108911156654358\n",
      "Steps: 213 Loss: 0.3660564720630646, Acc: 0.9108911156654358\n",
      "Steps: 214 Loss: 0.3648969531059265, Acc: 0.9108911156654358\n",
      "Steps: 215 Loss: 0.3637455105781555, Acc: 0.9108911156654358\n",
      "Steps: 216 Loss: 0.36260196566581726, Acc: 0.9207921028137207\n",
      "Steps: 217 Loss: 0.36146634817123413, Acc: 0.9207921028137207\n",
      "Steps: 218 Loss: 0.3603384494781494, Acc: 0.9207921028137207\n",
      "Steps: 219 Loss: 0.35921815037727356, Acc: 0.9207921028137207\n",
      "Steps: 220 Loss: 0.35810551047325134, Acc: 0.9207921028137207\n",
      "Steps: 221 Loss: 0.3570001721382141, Acc: 0.9207921028137207\n",
      "Steps: 222 Loss: 0.3559023439884186, Acc: 0.9306930899620056\n",
      "Steps: 223 Loss: 0.35481178760528564, Acc: 0.9306930899620056\n",
      "Steps: 224 Loss: 0.35372841358184814, Acc: 0.9306930899620056\n",
      "Steps: 225 Loss: 0.3526522219181061, Acc: 0.9306930899620056\n",
      "Steps: 226 Loss: 0.3515830934047699, Acc: 0.9306930899620056\n",
      "Steps: 227 Loss: 0.35052090883255005, Acc: 0.9306930899620056\n",
      "Steps: 228 Loss: 0.349465548992157, Acc: 0.9306930899620056\n",
      "Steps: 229 Loss: 0.34841710329055786, Acc: 0.9306930899620056\n",
      "Steps: 230 Loss: 0.3473753333091736, Acc: 0.9306930899620056\n",
      "Steps: 231 Loss: 0.34634026885032654, Acc: 0.9306930899620056\n",
      "Steps: 232 Loss: 0.3453117311000824, Acc: 0.9306930899620056\n",
      "Steps: 233 Loss: 0.34428974986076355, Acc: 0.9306930899620056\n",
      "Steps: 234 Loss: 0.34327417612075806, Acc: 0.9306930899620056\n",
      "Steps: 235 Loss: 0.34226498007774353, Acc: 0.9306930899620056\n",
      "Steps: 236 Loss: 0.3412621319293976, Acc: 0.9306930899620056\n",
      "Steps: 237 Loss: 0.34026551246643066, Acc: 0.9306930899620056\n",
      "Steps: 238 Loss: 0.3392750322818756, Acc: 0.9306930899620056\n",
      "Steps: 239 Loss: 0.3382906913757324, Acc: 0.9306930899620056\n",
      "Steps: 240 Loss: 0.33731234073638916, Acc: 0.9306930899620056\n",
      "Steps: 241 Loss: 0.3363399803638458, Acc: 0.9306930899620056\n",
      "Steps: 242 Loss: 0.33537352085113525, Acc: 0.9306930899620056\n",
      "Steps: 243 Loss: 0.33441290259361267, Acc: 0.9306930899620056\n",
      "Steps: 244 Loss: 0.3334581255912781, Acc: 0.9306930899620056\n",
      "Steps: 245 Loss: 0.33250898122787476, Acc: 0.9306930899620056\n",
      "Steps: 246 Loss: 0.33156561851501465, Acc: 0.9306930899620056\n",
      "Steps: 247 Loss: 0.33062776923179626, Acc: 0.9306930899620056\n",
      "Steps: 248 Loss: 0.32969552278518677, Acc: 0.9306930899620056\n",
      "Steps: 249 Loss: 0.3287687301635742, Acc: 0.9306930899620056\n",
      "Steps: 250 Loss: 0.3278473913669586, Acc: 0.9306930899620056\n",
      "Steps: 251 Loss: 0.3269314765930176, Acc: 0.9306930899620056\n",
      "Steps: 252 Loss: 0.32602089643478394, Acc: 0.9306930899620056\n",
      "Steps: 253 Loss: 0.3251156210899353, Acc: 0.9306930899620056\n",
      "Steps: 254 Loss: 0.32421553134918213, Acc: 0.9306930899620056\n",
      "Steps: 255 Loss: 0.3233206272125244, Acc: 0.9306930899620056\n",
      "Steps: 256 Loss: 0.322430819272995, Acc: 0.9306930899620056\n",
      "Steps: 257 Loss: 0.32154616713523865, Acc: 0.9306930899620056\n",
      "Steps: 258 Loss: 0.32066643238067627, Acc: 0.9306930899620056\n",
      "Steps: 259 Loss: 0.3197917342185974, Acc: 0.9306930899620056\n",
      "Steps: 260 Loss: 0.3189219832420349, Acc: 0.9306930899620056\n",
      "Steps: 261 Loss: 0.3180571496486664, Acc: 0.9306930899620056\n",
      "Steps: 262 Loss: 0.31719711422920227, Acc: 0.9306930899620056\n",
      "Steps: 263 Loss: 0.3163418471813202, Acc: 0.9306930899620056\n",
      "Steps: 264 Loss: 0.31549134850502014, Acc: 0.9306930899620056\n",
      "Steps: 265 Loss: 0.3146454989910126, Acc: 0.9306930899620056\n",
      "Steps: 266 Loss: 0.3138044476509094, Acc: 0.9306930899620056\n",
      "Steps: 267 Loss: 0.3129678964614868, Acc: 0.9306930899620056\n",
      "Steps: 268 Loss: 0.3121359348297119, Acc: 0.9306930899620056\n",
      "Steps: 269 Loss: 0.31130850315093994, Acc: 0.9306930899620056\n",
      "Steps: 270 Loss: 0.3104855716228485, Acc: 0.9306930899620056\n",
      "Steps: 271 Loss: 0.30966711044311523, Acc: 0.9306930899620056\n",
      "Steps: 272 Loss: 0.30885300040245056, Acc: 0.9306930899620056\n",
      "Steps: 273 Loss: 0.30804330110549927, Acc: 0.9306930899620056\n",
      "Steps: 274 Loss: 0.30723801255226135, Acc: 0.9306930899620056\n",
      "Steps: 275 Loss: 0.3064368963241577, Acc: 0.9306930899620056\n",
      "Steps: 276 Loss: 0.3056400716304779, Acc: 0.9306930899620056\n",
      "Steps: 277 Loss: 0.30484744906425476, Acc: 0.9306930899620056\n",
      "Steps: 278 Loss: 0.3040590286254883, Acc: 0.9306930899620056\n",
      "Steps: 279 Loss: 0.3032747507095337, Acc: 0.9306930899620056\n",
      "Steps: 280 Loss: 0.30249452590942383, Acc: 0.9306930899620056\n",
      "Steps: 281 Loss: 0.30171844363212585, Acc: 0.9306930899620056\n",
      "Steps: 282 Loss: 0.3009463846683502, Acc: 0.9306930899620056\n",
      "Steps: 283 Loss: 0.3001782298088074, Acc: 0.9306930899620056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 284 Loss: 0.29941415786743164, Acc: 0.9306930899620056\n",
      "Steps: 285 Loss: 0.29865390062332153, Acc: 0.9306930899620056\n",
      "Steps: 286 Loss: 0.29789769649505615, Acc: 0.9306930899620056\n",
      "Steps: 287 Loss: 0.2971452474594116, Acc: 0.9306930899620056\n",
      "Steps: 288 Loss: 0.2963966429233551, Acc: 0.9306930899620056\n",
      "Steps: 289 Loss: 0.2956518530845642, Acc: 0.9306930899620056\n",
      "Steps: 290 Loss: 0.29491081833839417, Acc: 0.9306930899620056\n",
      "Steps: 291 Loss: 0.29417353868484497, Acc: 0.9306930899620056\n",
      "Steps: 292 Loss: 0.29343995451927185, Acc: 0.9306930899620056\n",
      "Steps: 293 Loss: 0.2927100360393524, Acc: 0.9306930899620056\n",
      "Steps: 294 Loss: 0.2919837534427643, Acc: 0.9405940771102905\n",
      "Steps: 295 Loss: 0.29126113653182983, Acc: 0.9405940771102905\n",
      "Steps: 296 Loss: 0.29054203629493713, Acc: 0.9405940771102905\n",
      "Steps: 297 Loss: 0.28982648253440857, Acc: 0.9405940771102905\n",
      "Steps: 298 Loss: 0.2891145348548889, Acc: 0.9405940771102905\n",
      "Steps: 299 Loss: 0.28840604424476624, Acc: 0.9405940771102905\n",
      "Steps: 300 Loss: 0.2877010405063629, Acc: 0.9405940771102905\n",
      "Steps: 301 Loss: 0.2869994342327118, Acc: 0.9405940771102905\n",
      "Steps: 302 Loss: 0.28630131483078003, Acc: 0.9405940771102905\n",
      "Steps: 303 Loss: 0.2856065034866333, Acc: 0.9405940771102905\n",
      "Steps: 304 Loss: 0.28491508960723877, Acc: 0.9405940771102905\n",
      "Steps: 305 Loss: 0.2842269837856293, Acc: 0.9405940771102905\n",
      "Steps: 306 Loss: 0.28354230523109436, Acc: 0.9405940771102905\n",
      "Steps: 307 Loss: 0.28286075592041016, Acc: 0.9405940771102905\n",
      "Steps: 308 Loss: 0.2821825444698334, Acc: 0.9405940771102905\n",
      "Steps: 309 Loss: 0.28150755167007446, Acc: 0.9405940771102905\n",
      "Steps: 310 Loss: 0.2808357775211334, Acc: 0.9405940771102905\n",
      "Steps: 311 Loss: 0.2801671326160431, Acc: 0.9405940771102905\n",
      "Steps: 312 Loss: 0.27950170636177063, Acc: 0.9405940771102905\n",
      "Steps: 313 Loss: 0.2788393795490265, Acc: 0.9405940771102905\n",
      "Steps: 314 Loss: 0.27818015217781067, Acc: 0.9405940771102905\n",
      "Steps: 315 Loss: 0.27752405405044556, Acc: 0.9405940771102905\n",
      "Steps: 316 Loss: 0.2768709659576416, Acc: 0.9405940771102905\n",
      "Steps: 317 Loss: 0.2762209177017212, Acc: 0.9405940771102905\n",
      "Steps: 318 Loss: 0.2755739390850067, Acc: 0.9405940771102905\n",
      "Steps: 319 Loss: 0.2749299108982086, Acc: 0.9405940771102905\n",
      "Steps: 320 Loss: 0.2742888629436493, Acc: 0.9405940771102905\n",
      "Steps: 321 Loss: 0.27365073561668396, Acc: 0.9405940771102905\n",
      "Steps: 322 Loss: 0.2730155289173126, Acc: 0.9405940771102905\n",
      "Steps: 323 Loss: 0.27238330245018005, Acc: 0.9405940771102905\n",
      "Steps: 324 Loss: 0.2717539668083191, Acc: 0.9504950642585754\n",
      "Steps: 325 Loss: 0.2711274027824402, Acc: 0.9504950642585754\n",
      "Steps: 326 Loss: 0.2705037295818329, Acc: 0.9504950642585754\n",
      "Steps: 327 Loss: 0.2698828876018524, Acc: 0.9504950642585754\n",
      "Steps: 328 Loss: 0.269264817237854, Acc: 0.9504950642585754\n",
      "Steps: 329 Loss: 0.2686495780944824, Acc: 0.9504950642585754\n",
      "Steps: 330 Loss: 0.2680370509624481, Acc: 0.9504950642585754\n",
      "Steps: 331 Loss: 0.2674272656440735, Acc: 0.9504950642585754\n",
      "Steps: 332 Loss: 0.26682016253471375, Acc: 0.9504950642585754\n",
      "Steps: 333 Loss: 0.26621583104133606, Acc: 0.9504950642585754\n",
      "Steps: 334 Loss: 0.2656141519546509, Acc: 0.9504950642585754\n",
      "Steps: 335 Loss: 0.2650151252746582, Acc: 0.9504950642585754\n",
      "Steps: 336 Loss: 0.26441875100135803, Acc: 0.9504950642585754\n",
      "Steps: 337 Loss: 0.263824999332428, Acc: 0.9504950642585754\n",
      "Steps: 338 Loss: 0.26323390007019043, Acc: 0.9504950642585754\n",
      "Steps: 339 Loss: 0.26264533400535583, Acc: 0.9504950642585754\n",
      "Steps: 340 Loss: 0.26205939054489136, Acc: 0.9504950642585754\n",
      "Steps: 341 Loss: 0.26147598028182983, Acc: 0.9504950642585754\n",
      "Steps: 342 Loss: 0.2608950734138489, Acc: 0.9504950642585754\n",
      "Steps: 343 Loss: 0.2603166699409485, Acc: 0.9504950642585754\n",
      "Steps: 344 Loss: 0.25974082946777344, Acc: 0.9504950642585754\n",
      "Steps: 345 Loss: 0.2591674029827118, Acc: 0.9504950642585754\n",
      "Steps: 346 Loss: 0.2585965096950531, Acc: 0.9504950642585754\n",
      "Steps: 347 Loss: 0.2580280601978302, Acc: 0.9504950642585754\n",
      "Steps: 348 Loss: 0.2574619948863983, Acc: 0.9504950642585754\n",
      "Steps: 349 Loss: 0.2568984031677246, Acc: 0.9504950642585754\n",
      "Steps: 350 Loss: 0.25633710622787476, Acc: 0.9504950642585754\n",
      "Steps: 351 Loss: 0.25577831268310547, Acc: 0.9504950642585754\n",
      "Steps: 352 Loss: 0.25522178411483765, Acc: 0.9504950642585754\n",
      "Steps: 353 Loss: 0.2546676695346832, Acc: 0.9504950642585754\n",
      "Steps: 354 Loss: 0.25411590933799744, Acc: 0.9504950642585754\n",
      "Steps: 355 Loss: 0.2535664439201355, Acc: 0.9504950642585754\n",
      "Steps: 356 Loss: 0.2530192732810974, Acc: 0.9504950642585754\n",
      "Steps: 357 Loss: 0.2524743974208832, Acc: 0.9504950642585754\n",
      "Steps: 358 Loss: 0.2519318461418152, Acc: 0.9504950642585754\n",
      "Steps: 359 Loss: 0.2513914704322815, Acc: 0.9504950642585754\n",
      "Steps: 360 Loss: 0.2508533298969269, Acc: 0.9504950642585754\n",
      "Steps: 361 Loss: 0.2503174841403961, Acc: 0.9504950642585754\n",
      "Steps: 362 Loss: 0.24978384375572205, Acc: 0.9504950642585754\n",
      "Steps: 363 Loss: 0.24925237894058228, Acc: 0.9504950642585754\n",
      "Steps: 364 Loss: 0.2487231194972992, Acc: 0.9504950642585754\n",
      "Steps: 365 Loss: 0.24819602072238922, Acc: 0.9504950642585754\n",
      "Steps: 366 Loss: 0.24767111241817474, Acc: 0.9504950642585754\n",
      "Steps: 367 Loss: 0.2471483051776886, Acc: 0.9504950642585754\n",
      "Steps: 368 Loss: 0.24662764370441437, Acc: 0.9504950642585754\n",
      "Steps: 369 Loss: 0.24610908329486847, Acc: 0.9504950642585754\n",
      "Steps: 370 Loss: 0.2455926239490509, Acc: 0.9504950642585754\n",
      "Steps: 371 Loss: 0.24507828056812286, Acc: 0.9504950642585754\n",
      "Steps: 372 Loss: 0.24456602334976196, Acc: 0.9504950642585754\n",
      "Steps: 373 Loss: 0.24405579268932343, Acc: 0.9504950642585754\n",
      "Steps: 374 Loss: 0.24354760348796844, Acc: 0.9504950642585754\n",
      "Steps: 375 Loss: 0.2430415004491806, Acc: 0.9504950642585754\n",
      "Steps: 376 Loss: 0.24253737926483154, Acc: 0.9504950642585754\n",
      "Steps: 377 Loss: 0.24203525483608246, Acc: 0.9504950642585754\n",
      "Steps: 378 Loss: 0.24153517186641693, Acc: 0.9504950642585754\n",
      "Steps: 379 Loss: 0.24103707075119019, Acc: 0.9504950642585754\n",
      "Steps: 380 Loss: 0.24054095149040222, Acc: 0.9504950642585754\n",
      "Steps: 381 Loss: 0.24004676938056946, Acc: 0.9504950642585754\n",
      "Steps: 382 Loss: 0.23955456912517548, Acc: 0.9504950642585754\n",
      "Steps: 383 Loss: 0.2390643060207367, Acc: 0.9504950642585754\n",
      "Steps: 384 Loss: 0.23857592046260834, Acc: 0.9504950642585754\n",
      "Steps: 385 Loss: 0.2380894422531128, Acc: 0.9504950642585754\n",
      "Steps: 386 Loss: 0.23760490119457245, Acc: 0.9504950642585754\n",
      "Steps: 387 Loss: 0.23712226748466492, Acc: 0.9504950642585754\n",
      "Steps: 388 Loss: 0.23664149641990662, Acc: 0.9504950642585754\n",
      "Steps: 389 Loss: 0.23616258800029755, Acc: 0.9504950642585754\n",
      "Steps: 390 Loss: 0.2356855869293213, Acc: 0.9504950642585754\n",
      "Steps: 391 Loss: 0.2352103441953659, Acc: 0.9504950642585754\n",
      "Steps: 392 Loss: 0.23473699390888214, Acc: 0.9504950642585754\n",
      "Steps: 393 Loss: 0.23426543176174164, Acc: 0.9504950642585754\n",
      "Steps: 394 Loss: 0.23379573225975037, Acc: 0.9504950642585754\n",
      "Steps: 395 Loss: 0.2333277314901352, Acc: 0.9504950642585754\n",
      "Steps: 396 Loss: 0.23286160826683044, Acc: 0.9504950642585754\n",
      "Steps: 397 Loss: 0.23239728808403015, Acc: 0.9504950642585754\n",
      "Steps: 398 Loss: 0.23193465173244476, Acc: 0.9504950642585754\n",
      "Steps: 399 Loss: 0.23147383332252502, Acc: 0.9504950642585754\n",
      "Steps: 400 Loss: 0.23101474344730377, Acc: 0.9504950642585754\n",
      "Steps: 401 Loss: 0.2305574119091034, Acc: 0.9504950642585754\n",
      "Steps: 402 Loss: 0.2301017791032791, Acc: 0.9504950642585754\n",
      "Steps: 403 Loss: 0.22964785993099213, Acc: 0.9504950642585754\n",
      "Steps: 404 Loss: 0.22919565439224243, Acc: 0.9504950642585754\n",
      "Steps: 405 Loss: 0.22874513268470764, Acc: 0.9504950642585754\n",
      "Steps: 406 Loss: 0.22829633951187134, Acc: 0.9504950642585754\n",
      "Steps: 407 Loss: 0.22784923017024994, Acc: 0.9504950642585754\n",
      "Steps: 408 Loss: 0.22740375995635986, Acc: 0.9504950642585754\n",
      "Steps: 409 Loss: 0.2269599288702011, Acc: 0.9504950642585754\n",
      "Steps: 410 Loss: 0.22651773691177368, Acc: 0.9504950642585754\n",
      "Steps: 411 Loss: 0.22607725858688354, Acc: 0.9504950642585754\n",
      "Steps: 412 Loss: 0.22563835978507996, Acc: 0.9504950642585754\n",
      "Steps: 413 Loss: 0.2252010852098465, Acc: 0.9504950642585754\n",
      "Steps: 414 Loss: 0.22476547956466675, Acc: 0.9504950642585754\n",
      "Steps: 415 Loss: 0.22433139383792877, Acc: 0.9504950642585754\n",
      "Steps: 416 Loss: 0.22389891743659973, Acc: 0.9504950642585754\n",
      "Steps: 417 Loss: 0.22346802055835724, Acc: 0.9504950642585754\n",
      "Steps: 418 Loss: 0.2230387181043625, Acc: 0.9504950642585754\n",
      "Steps: 419 Loss: 0.2226109653711319, Acc: 0.9504950642585754\n",
      "Steps: 420 Loss: 0.22218480706214905, Acc: 0.9504950642585754\n",
      "Steps: 421 Loss: 0.22176021337509155, Acc: 0.9504950642585754\n",
      "Steps: 422 Loss: 0.22133712470531464, Acc: 0.9504950642585754\n",
      "Steps: 423 Loss: 0.22091546654701233, Acc: 0.9504950642585754\n",
      "Steps: 424 Loss: 0.22049544751644135, Acc: 0.9504950642585754\n",
      "Steps: 425 Loss: 0.22007693350315094, Acc: 0.9504950642585754\n",
      "Steps: 426 Loss: 0.21965989470481873, Acc: 0.9504950642585754\n",
      "Steps: 427 Loss: 0.21924442052841187, Acc: 0.9504950642585754\n",
      "Steps: 428 Loss: 0.21883034706115723, Acc: 0.9504950642585754\n",
      "Steps: 429 Loss: 0.21841782331466675, Acc: 0.9504950642585754\n",
      "Steps: 430 Loss: 0.2180067002773285, Acc: 0.9504950642585754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 431 Loss: 0.217597097158432, Acc: 0.9504950642585754\n",
      "Steps: 432 Loss: 0.21718892455101013, Acc: 0.9504950642585754\n",
      "Steps: 433 Loss: 0.21678219735622406, Acc: 0.9504950642585754\n",
      "Steps: 434 Loss: 0.21637693047523499, Acc: 0.9504950642585754\n",
      "Steps: 435 Loss: 0.21597306430339813, Acc: 0.9504950642585754\n",
      "Steps: 436 Loss: 0.21557065844535828, Acc: 0.9504950642585754\n",
      "Steps: 437 Loss: 0.21516968309879303, Acc: 0.9504950642585754\n",
      "Steps: 438 Loss: 0.2147700935602188, Acc: 0.9504950642585754\n",
      "Steps: 439 Loss: 0.21437190473079681, Acc: 0.9504950642585754\n",
      "Steps: 440 Loss: 0.21397510170936584, Acc: 0.9504950642585754\n",
      "Steps: 441 Loss: 0.2135796844959259, Acc: 0.9504950642585754\n",
      "Steps: 442 Loss: 0.2131856381893158, Acc: 0.9504950642585754\n",
      "Steps: 443 Loss: 0.2127930223941803, Acc: 0.9504950642585754\n",
      "Steps: 444 Loss: 0.21240171790122986, Acc: 0.9504950642585754\n",
      "Steps: 445 Loss: 0.21201175451278687, Acc: 0.9504950642585754\n",
      "Steps: 446 Loss: 0.2116232067346573, Acc: 0.9504950642585754\n",
      "Steps: 447 Loss: 0.21123595535755157, Acc: 0.9504950642585754\n",
      "Steps: 448 Loss: 0.2108500599861145, Acc: 0.9504950642585754\n",
      "Steps: 449 Loss: 0.2104654759168625, Acc: 0.9504950642585754\n",
      "Steps: 450 Loss: 0.21008223295211792, Acc: 0.9504950642585754\n",
      "Steps: 451 Loss: 0.20970025658607483, Acc: 0.9504950642585754\n",
      "Steps: 452 Loss: 0.20931963622570038, Acc: 0.9504950642585754\n",
      "Steps: 453 Loss: 0.2089402973651886, Acc: 0.9504950642585754\n",
      "Steps: 454 Loss: 0.20856226980686188, Acc: 0.9504950642585754\n",
      "Steps: 455 Loss: 0.20818549394607544, Acc: 0.9504950642585754\n",
      "Steps: 456 Loss: 0.20781005918979645, Acc: 0.9504950642585754\n",
      "Steps: 457 Loss: 0.20743584632873535, Acc: 0.9504950642585754\n",
      "Steps: 458 Loss: 0.20706290006637573, Acc: 0.9504950642585754\n",
      "Steps: 459 Loss: 0.20669123530387878, Acc: 0.9504950642585754\n",
      "Steps: 460 Loss: 0.20632079243659973, Acc: 0.9504950642585754\n",
      "Steps: 461 Loss: 0.20595164597034454, Acc: 0.9504950642585754\n",
      "Steps: 462 Loss: 0.20558373630046844, Acc: 0.9504950642585754\n",
      "Steps: 463 Loss: 0.20521701872348785, Acc: 0.9504950642585754\n",
      "Steps: 464 Loss: 0.20485158264636993, Acc: 0.9504950642585754\n",
      "Steps: 465 Loss: 0.20448729395866394, Acc: 0.9504950642585754\n",
      "Steps: 466 Loss: 0.2041243016719818, Acc: 0.9504950642585754\n",
      "Steps: 467 Loss: 0.2037624716758728, Acc: 0.9504950642585754\n",
      "Steps: 468 Loss: 0.2034018635749817, Acc: 0.9504950642585754\n",
      "Steps: 469 Loss: 0.2030424326658249, Acc: 0.9504950642585754\n",
      "Steps: 470 Loss: 0.20268425345420837, Acc: 0.9603960514068604\n",
      "Steps: 471 Loss: 0.2023271769285202, Acc: 0.9603960514068604\n",
      "Steps: 472 Loss: 0.20197135210037231, Acc: 0.9603960514068604\n",
      "Steps: 473 Loss: 0.20161664485931396, Acc: 0.9603960514068604\n",
      "Steps: 474 Loss: 0.20126309990882874, Acc: 0.9603960514068604\n",
      "Steps: 475 Loss: 0.2009107917547226, Acc: 0.9603960514068604\n",
      "Steps: 476 Loss: 0.2005595564842224, Acc: 0.9603960514068604\n",
      "Steps: 477 Loss: 0.20020951330661774, Acc: 0.9603960514068604\n",
      "Steps: 478 Loss: 0.19986063241958618, Acc: 0.9603960514068604\n",
      "Steps: 479 Loss: 0.19951285421848297, Acc: 0.9603960514068604\n",
      "Steps: 480 Loss: 0.19916623830795288, Acc: 0.9603960514068604\n",
      "Steps: 481 Loss: 0.19882073998451233, Acc: 0.9603960514068604\n",
      "Steps: 482 Loss: 0.19847634434700012, Acc: 0.9603960514068604\n",
      "Steps: 483 Loss: 0.19813306629657745, Acc: 0.9603960514068604\n",
      "Steps: 484 Loss: 0.19779090583324432, Acc: 0.9603960514068604\n",
      "Steps: 485 Loss: 0.19744984805583954, Acc: 0.9603960514068604\n",
      "Steps: 486 Loss: 0.1971099078655243, Acc: 0.9603960514068604\n",
      "Steps: 487 Loss: 0.1967710703611374, Acc: 0.9603960514068604\n",
      "Steps: 488 Loss: 0.19643324613571167, Acc: 0.9603960514068604\n",
      "Steps: 489 Loss: 0.19609661400318146, Acc: 0.9603960514068604\n",
      "Steps: 490 Loss: 0.19576096534729004, Acc: 0.9603960514068604\n",
      "Steps: 491 Loss: 0.19542643427848816, Acc: 0.9603960514068604\n",
      "Steps: 492 Loss: 0.19509299099445343, Acc: 0.9603960514068604\n",
      "Steps: 493 Loss: 0.19476057589054108, Acc: 0.9603960514068604\n",
      "Steps: 494 Loss: 0.19442927837371826, Acc: 0.9603960514068604\n",
      "Steps: 495 Loss: 0.19409897923469543, Acc: 0.9603960514068604\n",
      "Steps: 496 Loss: 0.1937696933746338, Acc: 0.9603960514068604\n",
      "Steps: 497 Loss: 0.1934414803981781, Acc: 0.9603960514068604\n",
      "Steps: 498 Loss: 0.19311434030532837, Acc: 0.9603960514068604\n",
      "Steps: 499 Loss: 0.19278821349143982, Acc: 0.9603960514068604\n",
      "Steps: 500 Loss: 0.19246312975883484, Acc: 0.9603960514068604\n",
      "Steps: 501 Loss: 0.19213901460170746, Acc: 0.9603960514068604\n",
      "Steps: 502 Loss: 0.19181597232818604, Acc: 0.9603960514068604\n",
      "Steps: 503 Loss: 0.1914939135313034, Acc: 0.9603960514068604\n",
      "Steps: 504 Loss: 0.19117283821105957, Acc: 0.9603960514068604\n",
      "Steps: 505 Loss: 0.1908528208732605, Acc: 0.9603960514068604\n",
      "Steps: 506 Loss: 0.19053378701210022, Acc: 0.9603960514068604\n",
      "Steps: 507 Loss: 0.19021576642990112, Acc: 0.9603960514068604\n",
      "Steps: 508 Loss: 0.18989868462085724, Acc: 0.9603960514068604\n",
      "Steps: 509 Loss: 0.18958263099193573, Acc: 0.9603960514068604\n",
      "Steps: 510 Loss: 0.18926754593849182, Acc: 0.9603960514068604\n",
      "Steps: 511 Loss: 0.18895339965820312, Acc: 0.9603960514068604\n",
      "Steps: 512 Loss: 0.1886402666568756, Acc: 0.9603960514068604\n",
      "Steps: 513 Loss: 0.1883281171321869, Acc: 0.9603960514068604\n",
      "Steps: 514 Loss: 0.188016876578331, Acc: 0.9603960514068604\n",
      "Steps: 515 Loss: 0.18770663440227509, Acc: 0.9603960514068604\n",
      "Steps: 516 Loss: 0.1873973309993744, Acc: 0.9603960514068604\n",
      "Steps: 517 Loss: 0.18708892166614532, Acc: 0.9603960514068604\n",
      "Steps: 518 Loss: 0.18678157031536102, Acc: 0.9603960514068604\n",
      "Steps: 519 Loss: 0.18647506833076477, Acc: 0.9603960514068604\n",
      "Steps: 520 Loss: 0.18616952002048492, Acc: 0.9603960514068604\n",
      "Steps: 521 Loss: 0.1858649104833603, Acc: 0.9603960514068604\n",
      "Steps: 522 Loss: 0.18556126952171326, Acc: 0.9603960514068604\n",
      "Steps: 523 Loss: 0.18525844812393188, Acc: 0.9603960514068604\n",
      "Steps: 524 Loss: 0.1849566549062729, Acc: 0.9603960514068604\n",
      "Steps: 525 Loss: 0.18465571105480194, Acc: 0.9603960514068604\n",
      "Steps: 526 Loss: 0.18435567617416382, Acc: 0.9603960514068604\n",
      "Steps: 527 Loss: 0.1840566098690033, Acc: 0.9603960514068604\n",
      "Steps: 528 Loss: 0.18375839293003082, Acc: 0.9603960514068604\n",
      "Steps: 529 Loss: 0.18346109986305237, Acc: 0.9603960514068604\n",
      "Steps: 530 Loss: 0.18316467106342316, Acc: 0.9603960514068604\n",
      "Steps: 531 Loss: 0.18286915123462677, Acc: 0.9603960514068604\n",
      "Steps: 532 Loss: 0.18257452547550201, Acc: 0.9603960514068604\n",
      "Steps: 533 Loss: 0.1822807639837265, Acc: 0.9603960514068604\n",
      "Steps: 534 Loss: 0.18198788166046143, Acc: 0.9603960514068604\n",
      "Steps: 535 Loss: 0.1816958636045456, Acc: 0.9603960514068604\n",
      "Steps: 536 Loss: 0.1814046949148178, Acc: 0.9603960514068604\n",
      "Steps: 537 Loss: 0.18111447989940643, Acc: 0.9603960514068604\n",
      "Steps: 538 Loss: 0.18082503974437714, Acc: 0.9603960514068604\n",
      "Steps: 539 Loss: 0.18053650856018066, Acc: 0.9603960514068604\n",
      "Steps: 540 Loss: 0.18024887144565582, Acc: 0.9603960514068604\n",
      "Steps: 541 Loss: 0.17996199429035187, Acc: 0.9603960514068604\n",
      "Steps: 542 Loss: 0.17967598140239716, Acc: 0.9603960514068604\n",
      "Steps: 543 Loss: 0.1793908178806305, Acc: 0.9603960514068604\n",
      "Steps: 544 Loss: 0.17910651862621307, Acc: 0.9603960514068604\n",
      "Steps: 545 Loss: 0.17882303893566132, Acc: 0.9603960514068604\n",
      "Steps: 546 Loss: 0.1785404086112976, Acc: 0.9603960514068604\n",
      "Steps: 547 Loss: 0.17825855314731598, Acc: 0.9603960514068604\n",
      "Steps: 548 Loss: 0.17797759175300598, Acc: 0.9603960514068604\n",
      "Steps: 549 Loss: 0.17769743502140045, Acc: 0.9603960514068604\n",
      "Steps: 550 Loss: 0.1774180829524994, Acc: 0.9603960514068604\n",
      "Steps: 551 Loss: 0.1771395206451416, Acc: 0.9603960514068604\n",
      "Steps: 552 Loss: 0.17686179280281067, Acc: 0.9603960514068604\n",
      "Steps: 553 Loss: 0.1765848994255066, Acc: 0.9603960514068604\n",
      "Steps: 554 Loss: 0.1763087511062622, Acc: 0.9603960514068604\n",
      "Steps: 555 Loss: 0.17603343725204468, Acc: 0.9603960514068604\n",
      "Steps: 556 Loss: 0.17575892806053162, Acc: 0.9603960514068604\n",
      "Steps: 557 Loss: 0.17548523843288422, Acc: 0.9603960514068604\n",
      "Steps: 558 Loss: 0.1752123087644577, Acc: 0.9603960514068604\n",
      "Steps: 559 Loss: 0.17494015395641327, Acc: 0.9603960514068604\n",
      "Steps: 560 Loss: 0.1746687889099121, Acc: 0.9603960514068604\n",
      "Steps: 561 Loss: 0.17439818382263184, Acc: 0.9603960514068604\n",
      "Steps: 562 Loss: 0.17412838339805603, Acc: 0.9603960514068604\n",
      "Steps: 563 Loss: 0.1738593727350235, Acc: 0.9603960514068604\n",
      "Steps: 564 Loss: 0.17359106242656708, Acc: 0.9603960514068604\n",
      "Steps: 565 Loss: 0.1733236014842987, Acc: 0.9603960514068604\n",
      "Steps: 566 Loss: 0.17305685579776764, Acc: 0.9603960514068604\n",
      "Steps: 567 Loss: 0.17279087007045746, Acc: 0.9603960514068604\n",
      "Steps: 568 Loss: 0.17252567410469055, Acc: 0.9603960514068604\n",
      "Steps: 569 Loss: 0.17226113379001617, Acc: 0.9603960514068604\n",
      "Steps: 570 Loss: 0.17199747264385223, Acc: 0.9603960514068604\n",
      "Steps: 571 Loss: 0.1717344969511032, Acc: 0.9603960514068604\n",
      "Steps: 572 Loss: 0.17147231101989746, Acc: 0.9603960514068604\n",
      "Steps: 573 Loss: 0.17121081054210663, Acc: 0.9603960514068604\n",
      "Steps: 574 Loss: 0.17095007002353668, Acc: 0.9603960514068604\n",
      "Steps: 575 Loss: 0.17069005966186523, Acc: 0.9603960514068604\n",
      "Steps: 576 Loss: 0.17043077945709229, Acc: 0.9603960514068604\n",
      "Steps: 577 Loss: 0.17017222940921783, Acc: 0.9603960514068604\n",
      "Steps: 578 Loss: 0.1699143946170807, Acc: 0.9603960514068604\n",
      "Steps: 579 Loss: 0.16965727508068085, Acc: 0.9603960514068604\n",
      "Steps: 580 Loss: 0.16940094530582428, Acc: 0.9603960514068604\n",
      "Steps: 581 Loss: 0.16914528608322144, Acc: 0.9603960514068604\n",
      "Steps: 582 Loss: 0.16889029741287231, Acc: 0.9603960514068604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 583 Loss: 0.16863608360290527, Acc: 0.9603960514068604\n",
      "Steps: 584 Loss: 0.16838255524635315, Acc: 0.9603960514068604\n",
      "Steps: 585 Loss: 0.16812972724437714, Acc: 0.9603960514068604\n",
      "Steps: 586 Loss: 0.16787761449813843, Acc: 0.9603960514068604\n",
      "Steps: 587 Loss: 0.16762621700763702, Acc: 0.9603960514068604\n",
      "Steps: 588 Loss: 0.16737541556358337, Acc: 0.9603960514068604\n",
      "Steps: 589 Loss: 0.167125403881073, Acc: 0.9603960514068604\n",
      "Steps: 590 Loss: 0.16687606275081635, Acc: 0.9603960514068604\n",
      "Steps: 591 Loss: 0.166627436876297, Acc: 0.9603960514068604\n",
      "Steps: 592 Loss: 0.16637945175170898, Acc: 0.9603960514068604\n",
      "Steps: 593 Loss: 0.1661321371793747, Acc: 0.9603960514068604\n",
      "Steps: 594 Loss: 0.16588559746742249, Acc: 0.9603960514068604\n",
      "Steps: 595 Loss: 0.16563963890075684, Acc: 0.9603960514068604\n",
      "Steps: 596 Loss: 0.1653943955898285, Acc: 0.9603960514068604\n",
      "Steps: 597 Loss: 0.1651497632265091, Acc: 0.9603960514068604\n",
      "Steps: 598 Loss: 0.1649058759212494, Acc: 0.9603960514068604\n",
      "Steps: 599 Loss: 0.16466261446475983, Acc: 0.9603960514068604\n",
      "Steps: 600 Loss: 0.16442003846168518, Acc: 0.9603960514068604\n",
      "Steps: 601 Loss: 0.16417808830738068, Acc: 0.9603960514068604\n",
      "Steps: 602 Loss: 0.16393683850765228, Acc: 0.9603960514068604\n",
      "Steps: 603 Loss: 0.16369624435901642, Acc: 0.9603960514068604\n",
      "Steps: 604 Loss: 0.1634562909603119, Acc: 0.9603960514068604\n",
      "Steps: 605 Loss: 0.1632169783115387, Acc: 0.9603960514068604\n",
      "Steps: 606 Loss: 0.16297833621501923, Acc: 0.9603960514068604\n",
      "Steps: 607 Loss: 0.1627403348684311, Acc: 0.9603960514068604\n",
      "Steps: 608 Loss: 0.1625029444694519, Acc: 0.9603960514068604\n",
      "Steps: 609 Loss: 0.16226619482040405, Acc: 0.9603960514068604\n",
      "Steps: 610 Loss: 0.16203010082244873, Acc: 0.9603960514068604\n",
      "Steps: 611 Loss: 0.16179464757442474, Acc: 0.9603960514068604\n",
      "Steps: 612 Loss: 0.1615598499774933, Acc: 0.9603960514068604\n",
      "Steps: 613 Loss: 0.16132564842700958, Acc: 0.9603960514068604\n",
      "Steps: 614 Loss: 0.16109204292297363, Acc: 0.9603960514068604\n",
      "Steps: 615 Loss: 0.1608591228723526, Acc: 0.9603960514068604\n",
      "Steps: 616 Loss: 0.16062681376934052, Acc: 0.9603960514068604\n",
      "Steps: 617 Loss: 0.16039511561393738, Acc: 0.9603960514068604\n",
      "Steps: 618 Loss: 0.16016407310962677, Acc: 0.9603960514068604\n",
      "Steps: 619 Loss: 0.15993356704711914, Acc: 0.9603960514068604\n",
      "Steps: 620 Loss: 0.15970371663570404, Acc: 0.9603960514068604\n",
      "Steps: 621 Loss: 0.1594744771718979, Acc: 0.9603960514068604\n",
      "Steps: 622 Loss: 0.1592458188533783, Acc: 0.9603960514068604\n",
      "Steps: 623 Loss: 0.15901780128479004, Acc: 0.9603960514068604\n",
      "Steps: 624 Loss: 0.15879042446613312, Acc: 0.9603960514068604\n",
      "Steps: 625 Loss: 0.1585635393857956, Acc: 0.9603960514068604\n",
      "Steps: 626 Loss: 0.1583373248577118, Acc: 0.9603960514068604\n",
      "Steps: 627 Loss: 0.15811172127723694, Acc: 0.9603960514068604\n",
      "Steps: 628 Loss: 0.15788665413856506, Acc: 0.9603960514068604\n",
      "Steps: 629 Loss: 0.15766224265098572, Acc: 0.9603960514068604\n",
      "Steps: 630 Loss: 0.15743839740753174, Acc: 0.9603960514068604\n",
      "Steps: 631 Loss: 0.15721513330936432, Acc: 0.9603960514068604\n",
      "Steps: 632 Loss: 0.15699245035648346, Acc: 0.9603960514068604\n",
      "Steps: 633 Loss: 0.15677036345005035, Acc: 0.9603960514068604\n",
      "Steps: 634 Loss: 0.15654882788658142, Acc: 0.9603960514068604\n",
      "Steps: 635 Loss: 0.15632790327072144, Acc: 0.9603960514068604\n",
      "Steps: 636 Loss: 0.15610754489898682, Acc: 0.9603960514068604\n",
      "Steps: 637 Loss: 0.15588776767253876, Acc: 0.9603960514068604\n",
      "Steps: 638 Loss: 0.15566858649253845, Acc: 0.9603960514068604\n",
      "Steps: 639 Loss: 0.15544988214969635, Acc: 0.9603960514068604\n",
      "Steps: 640 Loss: 0.15523184835910797, Acc: 0.9603960514068604\n",
      "Steps: 641 Loss: 0.15501433610916138, Acc: 0.9603960514068604\n",
      "Steps: 642 Loss: 0.15479739010334015, Acc: 0.9603960514068604\n",
      "Steps: 643 Loss: 0.1545810103416443, Acc: 0.9603960514068604\n",
      "Steps: 644 Loss: 0.15436521172523499, Acc: 0.9603960514068604\n",
      "Steps: 645 Loss: 0.15414991974830627, Acc: 0.9603960514068604\n",
      "Steps: 646 Loss: 0.15393520891666412, Acc: 0.9603960514068604\n",
      "Steps: 647 Loss: 0.15372104942798615, Acc: 0.9603960514068604\n",
      "Steps: 648 Loss: 0.15350745618343353, Acc: 0.9603960514068604\n",
      "Steps: 649 Loss: 0.1532943993806839, Acc: 0.9603960514068604\n",
      "Steps: 650 Loss: 0.15308192372322083, Acc: 0.9603960514068604\n",
      "Steps: 651 Loss: 0.15286995470523834, Acc: 0.9603960514068604\n",
      "Steps: 652 Loss: 0.15265850722789764, Acc: 0.9603960514068604\n",
      "Steps: 653 Loss: 0.1524476557970047, Acc: 0.9603960514068604\n",
      "Steps: 654 Loss: 0.15223729610443115, Acc: 0.9603960514068604\n",
      "Steps: 655 Loss: 0.15202750265598297, Acc: 0.9603960514068604\n",
      "Steps: 656 Loss: 0.15181823074817657, Acc: 0.9603960514068604\n",
      "Steps: 657 Loss: 0.15160953998565674, Acc: 0.9603960514068604\n",
      "Steps: 658 Loss: 0.1514013111591339, Acc: 0.9603960514068604\n",
      "Steps: 659 Loss: 0.15119366347789764, Acc: 0.9603960514068604\n",
      "Steps: 660 Loss: 0.15098650753498077, Acc: 0.9603960514068604\n",
      "Steps: 661 Loss: 0.1507798433303833, Acc: 0.9603960514068604\n",
      "Steps: 662 Loss: 0.1505737453699112, Acc: 0.9603960514068604\n",
      "Steps: 663 Loss: 0.15036819875240326, Acc: 0.9603960514068604\n",
      "Steps: 664 Loss: 0.1501631736755371, Acc: 0.9603960514068604\n",
      "Steps: 665 Loss: 0.14995861053466797, Acc: 0.9603960514068604\n",
      "Steps: 666 Loss: 0.149754598736763, Acc: 0.9603960514068604\n",
      "Steps: 667 Loss: 0.14955106377601624, Acc: 0.9603960514068604\n",
      "Steps: 668 Loss: 0.14934806525707245, Acc: 0.9603960514068604\n",
      "Steps: 669 Loss: 0.14914560317993164, Acc: 0.9603960514068604\n",
      "Steps: 670 Loss: 0.14894361793994904, Acc: 0.9603960514068604\n",
      "Steps: 671 Loss: 0.1487421691417694, Acc: 0.9603960514068604\n",
      "Steps: 672 Loss: 0.1485411822795868, Acc: 0.9603960514068604\n",
      "Steps: 673 Loss: 0.14834073185920715, Acc: 0.9603960514068604\n",
      "Steps: 674 Loss: 0.14814074337482452, Acc: 0.9603960514068604\n",
      "Steps: 675 Loss: 0.14794130623340607, Acc: 0.9603960514068604\n",
      "Steps: 676 Loss: 0.14774233102798462, Acc: 0.9603960514068604\n",
      "Steps: 677 Loss: 0.14754386246204376, Acc: 0.9603960514068604\n",
      "Steps: 678 Loss: 0.1473458856344223, Acc: 0.9603960514068604\n",
      "Steps: 679 Loss: 0.14714844524860382, Acc: 0.9603960514068604\n",
      "Steps: 680 Loss: 0.14695143699645996, Acc: 0.9603960514068604\n",
      "Steps: 681 Loss: 0.1467549353837967, Acc: 0.9603960514068604\n",
      "Steps: 682 Loss: 0.14655892550945282, Acc: 0.9603960514068604\n",
      "Steps: 683 Loss: 0.14636337757110596, Acc: 0.9603960514068604\n",
      "Steps: 684 Loss: 0.1461683064699173, Acc: 0.9603960514068604\n",
      "Steps: 685 Loss: 0.14597375690937042, Acc: 0.9603960514068604\n",
      "Steps: 686 Loss: 0.14577968418598175, Acc: 0.9603960514068604\n",
      "Steps: 687 Loss: 0.1455860286951065, Acc: 0.9603960514068604\n",
      "Steps: 688 Loss: 0.14539292454719543, Acc: 0.9603960514068604\n",
      "Steps: 689 Loss: 0.14520029723644257, Acc: 0.9603960514068604\n",
      "Steps: 690 Loss: 0.1450081318616867, Acc: 0.9603960514068604\n",
      "Steps: 691 Loss: 0.14481636881828308, Acc: 0.9603960514068604\n",
      "Steps: 692 Loss: 0.14462515711784363, Acc: 0.9603960514068604\n",
      "Steps: 693 Loss: 0.1444343775510788, Acc: 0.9603960514068604\n",
      "Steps: 694 Loss: 0.14424410462379456, Acc: 0.9603960514068604\n",
      "Steps: 695 Loss: 0.14405426383018494, Acc: 0.9603960514068604\n",
      "Steps: 696 Loss: 0.14386485517024994, Acc: 0.9603960514068604\n",
      "Steps: 697 Loss: 0.14367596805095673, Acc: 0.9603960514068604\n",
      "Steps: 698 Loss: 0.14348752796649933, Acc: 0.9603960514068604\n",
      "Steps: 699 Loss: 0.14329953491687775, Acc: 0.9603960514068604\n",
      "Steps: 700 Loss: 0.14311203360557556, Acc: 0.9603960514068604\n",
      "Steps: 701 Loss: 0.14292491972446442, Acc: 0.9603960514068604\n",
      "Steps: 702 Loss: 0.14273828268051147, Acc: 0.9603960514068604\n",
      "Steps: 703 Loss: 0.14255215227603912, Acc: 0.9603960514068604\n",
      "Steps: 704 Loss: 0.1423664093017578, Acc: 0.9603960514068604\n",
      "Steps: 705 Loss: 0.1421811580657959, Acc: 0.9603960514068604\n",
      "Steps: 706 Loss: 0.1419963240623474, Acc: 0.9603960514068604\n",
      "Steps: 707 Loss: 0.14181195199489594, Acc: 0.9603960514068604\n",
      "Steps: 708 Loss: 0.14162804186344147, Acc: 0.9603960514068604\n",
      "Steps: 709 Loss: 0.14144451916217804, Acc: 0.9603960514068604\n",
      "Steps: 710 Loss: 0.1412615031003952, Acc: 0.9603960514068604\n",
      "Steps: 711 Loss: 0.1410788893699646, Acc: 0.9603960514068604\n",
      "Steps: 712 Loss: 0.1408967226743698, Acc: 0.9603960514068604\n",
      "Steps: 713 Loss: 0.14071498811244965, Acc: 0.9603960514068604\n",
      "Steps: 714 Loss: 0.1405336856842041, Acc: 0.9603960514068604\n",
      "Steps: 715 Loss: 0.14035280048847198, Acc: 0.9603960514068604\n",
      "Steps: 716 Loss: 0.14017240703105927, Acc: 0.9603960514068604\n",
      "Steps: 717 Loss: 0.1399923861026764, Acc: 0.9603960514068604\n",
      "Steps: 718 Loss: 0.13981284201145172, Acc: 0.9603960514068604\n",
      "Steps: 719 Loss: 0.13963370025157928, Acc: 0.9603960514068604\n",
      "Steps: 720 Loss: 0.13945500552654266, Acc: 0.9603960514068604\n",
      "Steps: 721 Loss: 0.13927672803401947, Acc: 0.9603960514068604\n",
      "Steps: 722 Loss: 0.1390988826751709, Acc: 0.9603960514068604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 723 Loss: 0.13892143964767456, Acc: 0.9603960514068604\n",
      "Steps: 724 Loss: 0.13874439895153046, Acc: 0.9603960514068604\n",
      "Steps: 725 Loss: 0.13856780529022217, Acc: 0.9603960514068604\n",
      "Steps: 726 Loss: 0.1383916288614273, Acc: 0.9603960514068604\n",
      "Steps: 727 Loss: 0.13821589946746826, Acc: 0.9603960514068604\n",
      "Steps: 728 Loss: 0.13804054260253906, Acc: 0.9603960514068604\n",
      "Steps: 729 Loss: 0.13786566257476807, Acc: 0.9603960514068604\n",
      "Steps: 730 Loss: 0.13769112527370453, Acc: 0.9603960514068604\n",
      "Steps: 731 Loss: 0.13751699030399323, Acc: 0.9603960514068604\n",
      "Steps: 732 Loss: 0.13734331727027893, Acc: 0.9603960514068604\n",
      "Steps: 733 Loss: 0.1371700018644333, Acc: 0.9603960514068604\n",
      "Steps: 734 Loss: 0.13699716329574585, Acc: 0.9603960514068604\n",
      "Steps: 735 Loss: 0.13682466745376587, Acc: 0.9603960514068604\n",
      "Steps: 736 Loss: 0.1366526037454605, Acc: 0.9603960514068604\n",
      "Steps: 737 Loss: 0.13648094236850739, Acc: 0.9603960514068604\n",
      "Steps: 738 Loss: 0.1363096684217453, Acc: 0.9603960514068604\n",
      "Steps: 739 Loss: 0.13613882660865784, Acc: 0.9603960514068604\n",
      "Steps: 740 Loss: 0.13596835732460022, Acc: 0.9603960514068604\n",
      "Steps: 741 Loss: 0.13579826056957245, Acc: 0.9603960514068604\n",
      "Steps: 742 Loss: 0.13562864065170288, Acc: 0.9603960514068604\n",
      "Steps: 743 Loss: 0.13545934855937958, Acc: 0.9603960514068604\n",
      "Steps: 744 Loss: 0.1352905035018921, Acc: 0.9603960514068604\n",
      "Steps: 745 Loss: 0.13512200117111206, Acc: 0.9603960514068604\n",
      "Steps: 746 Loss: 0.13495391607284546, Acc: 0.9603960514068604\n",
      "Steps: 747 Loss: 0.1347862035036087, Acc: 0.9603960514068604\n",
      "Steps: 748 Loss: 0.13461890816688538, Acc: 0.9603960514068604\n",
      "Steps: 749 Loss: 0.1344519555568695, Acc: 0.9603960514068604\n",
      "Steps: 750 Loss: 0.13428542017936707, Acc: 0.9603960514068604\n",
      "Steps: 751 Loss: 0.13411925733089447, Acc: 0.9603960514068604\n",
      "Steps: 752 Loss: 0.1339535415172577, Acc: 0.9603960514068604\n",
      "Steps: 753 Loss: 0.1337880939245224, Acc: 0.9603960514068604\n",
      "Steps: 754 Loss: 0.13362310826778412, Acc: 0.9603960514068604\n",
      "Steps: 755 Loss: 0.1334584802389145, Acc: 0.9603960514068604\n",
      "Steps: 756 Loss: 0.1332942545413971, Acc: 0.9603960514068604\n",
      "Steps: 757 Loss: 0.13313040137290955, Acc: 0.9603960514068604\n",
      "Steps: 758 Loss: 0.13296692073345184, Acc: 0.9603960514068604\n",
      "Steps: 759 Loss: 0.132803812623024, Acc: 0.9603960514068604\n",
      "Steps: 760 Loss: 0.13264107704162598, Acc: 0.9603960514068604\n",
      "Steps: 761 Loss: 0.13247869908809662, Acc: 0.9603960514068604\n",
      "Steps: 762 Loss: 0.1323167234659195, Acc: 0.9603960514068604\n",
      "Steps: 763 Loss: 0.13215507566928864, Acc: 0.9603960514068604\n",
      "Steps: 764 Loss: 0.1319938451051712, Acc: 0.9603960514068604\n",
      "Steps: 765 Loss: 0.13183295726776123, Acc: 0.9603960514068604\n",
      "Steps: 766 Loss: 0.1316724568605423, Acc: 0.9603960514068604\n",
      "Steps: 767 Loss: 0.13151229918003082, Acc: 0.9603960514068604\n",
      "Steps: 768 Loss: 0.1313524842262268, Acc: 0.9603960514068604\n",
      "Steps: 769 Loss: 0.13119304180145264, Acc: 0.9603960514068604\n",
      "Steps: 770 Loss: 0.1310339868068695, Acc: 0.9603960514068604\n",
      "Steps: 771 Loss: 0.13087528944015503, Acc: 0.9603960514068604\n",
      "Steps: 772 Loss: 0.1307169795036316, Acc: 0.9603960514068604\n",
      "Steps: 773 Loss: 0.13055899739265442, Acc: 0.9603960514068604\n",
      "Steps: 774 Loss: 0.1304013729095459, Acc: 0.9603960514068604\n",
      "Steps: 775 Loss: 0.13024410605430603, Acc: 0.9603960514068604\n",
      "Steps: 776 Loss: 0.13008719682693481, Acc: 0.9603960514068604\n",
      "Steps: 777 Loss: 0.12993064522743225, Acc: 0.9603960514068604\n",
      "Steps: 778 Loss: 0.12977443635463715, Acc: 0.9603960514068604\n",
      "Steps: 779 Loss: 0.1296185702085495, Acc: 0.9603960514068604\n",
      "Steps: 780 Loss: 0.1294631063938141, Acc: 0.9603960514068604\n",
      "Steps: 781 Loss: 0.12930791079998016, Acc: 0.9603960514068604\n",
      "Steps: 782 Loss: 0.12915313243865967, Acc: 0.9603960514068604\n",
      "Steps: 783 Loss: 0.12899869680404663, Acc: 0.9603960514068604\n",
      "Steps: 784 Loss: 0.12884458899497986, Acc: 0.9603960514068604\n",
      "Steps: 785 Loss: 0.12869080901145935, Acc: 0.9603960514068604\n",
      "Steps: 786 Loss: 0.1285373866558075, Acc: 0.9603960514068604\n",
      "Steps: 787 Loss: 0.1283843219280243, Acc: 0.9603960514068604\n",
      "Steps: 788 Loss: 0.12823159992694855, Acc: 0.9603960514068604\n",
      "Steps: 789 Loss: 0.12807923555374146, Acc: 0.9603960514068604\n",
      "Steps: 790 Loss: 0.12792718410491943, Acc: 0.9603960514068604\n",
      "Steps: 791 Loss: 0.12777547538280487, Acc: 0.9603960514068604\n",
      "Steps: 792 Loss: 0.12762406468391418, Acc: 0.9603960514068604\n",
      "Steps: 793 Loss: 0.12747307121753693, Acc: 0.9603960514068604\n",
      "Steps: 794 Loss: 0.12732236087322235, Acc: 0.9603960514068604\n",
      "Steps: 795 Loss: 0.12717197835445404, Acc: 0.9603960514068604\n",
      "Steps: 796 Loss: 0.12702198326587677, Acc: 0.9603960514068604\n",
      "Steps: 797 Loss: 0.126872256398201, Acc: 0.9603960514068604\n",
      "Steps: 798 Loss: 0.12672291696071625, Acc: 0.9603960514068604\n",
      "Steps: 799 Loss: 0.12657390534877777, Acc: 0.9603960514068604\n",
      "Steps: 800 Loss: 0.12642519176006317, Acc: 0.9603960514068604\n",
      "Steps: 801 Loss: 0.12627682089805603, Acc: 0.9603960514068604\n",
      "Steps: 802 Loss: 0.12612871825695038, Acc: 0.9603960514068604\n",
      "Steps: 803 Loss: 0.12598101794719696, Acc: 0.9603960514068604\n",
      "Steps: 804 Loss: 0.1258336454629898, Acc: 0.9603960514068604\n",
      "Steps: 805 Loss: 0.1256866157054901, Acc: 0.9603960514068604\n",
      "Steps: 806 Loss: 0.1255398541688919, Acc: 0.9603960514068604\n",
      "Steps: 807 Loss: 0.12539342045783997, Acc: 0.9603960514068604\n",
      "Steps: 808 Loss: 0.12524734437465668, Acc: 0.9603960514068604\n",
      "Steps: 809 Loss: 0.12510153651237488, Acc: 0.9603960514068604\n",
      "Steps: 810 Loss: 0.12495610117912292, Acc: 0.9603960514068604\n",
      "Steps: 811 Loss: 0.12481096386909485, Acc: 0.9603960514068604\n",
      "Steps: 812 Loss: 0.12466613948345184, Acc: 0.9603960514068604\n",
      "Steps: 813 Loss: 0.12452161312103271, Acc: 0.9603960514068604\n",
      "Steps: 814 Loss: 0.12437745183706284, Acc: 0.9603960514068604\n",
      "Steps: 815 Loss: 0.12423355132341385, Acc: 0.9603960514068604\n",
      "Steps: 816 Loss: 0.12409000843763351, Acc: 0.9603960514068604\n",
      "Steps: 817 Loss: 0.12394678592681885, Acc: 0.9603960514068604\n",
      "Steps: 818 Loss: 0.12380381673574448, Acc: 0.9603960514068604\n",
      "Steps: 819 Loss: 0.12366117537021637, Acc: 0.9603960514068604\n",
      "Steps: 820 Loss: 0.12351889908313751, Acc: 0.9603960514068604\n",
      "Steps: 821 Loss: 0.12337686866521835, Acc: 0.9603960514068604\n",
      "Steps: 822 Loss: 0.12323521077632904, Acc: 0.9603960514068604\n",
      "Steps: 823 Loss: 0.12309381365776062, Acc: 0.9603960514068604\n",
      "Steps: 824 Loss: 0.12295274436473846, Acc: 0.9603960514068604\n",
      "Steps: 825 Loss: 0.1228119507431984, Acc: 0.9603960514068604\n",
      "Steps: 826 Loss: 0.1226714700460434, Acc: 0.9603960514068604\n",
      "Steps: 827 Loss: 0.12253129482269287, Acc: 0.9603960514068604\n",
      "Steps: 828 Loss: 0.12239145487546921, Acc: 0.9603960514068604\n",
      "Steps: 829 Loss: 0.12225189059972763, Acc: 0.9603960514068604\n",
      "Steps: 830 Loss: 0.12211261689662933, Acc: 0.9603960514068604\n",
      "Steps: 831 Loss: 0.1219736784696579, Acc: 0.9603960514068604\n",
      "Steps: 832 Loss: 0.12183497846126556, Acc: 0.9801980257034302\n",
      "Steps: 833 Loss: 0.12169661372900009, Acc: 0.9801980257034302\n",
      "Steps: 834 Loss: 0.12155855447053909, Acc: 0.9801980257034302\n",
      "Steps: 835 Loss: 0.12142077833414078, Acc: 0.9801980257034302\n",
      "Steps: 836 Loss: 0.12128333002328873, Acc: 0.9801980257034302\n",
      "Steps: 837 Loss: 0.12114615738391876, Acc: 0.9801980257034302\n",
      "Steps: 838 Loss: 0.12100923806428909, Acc: 0.9801980257034302\n",
      "Steps: 839 Loss: 0.12087266892194748, Acc: 0.9801980257034302\n",
      "Steps: 840 Loss: 0.12073636054992676, Acc: 0.9801980257034302\n",
      "Steps: 841 Loss: 0.12060034275054932, Acc: 0.9801980257034302\n",
      "Steps: 842 Loss: 0.12046464532613754, Acc: 0.9801980257034302\n",
      "Steps: 843 Loss: 0.12032920867204666, Acc: 0.9801980257034302\n",
      "Steps: 844 Loss: 0.12019408494234085, Acc: 0.9801980257034302\n",
      "Steps: 845 Loss: 0.12005919963121414, Acc: 0.9801980257034302\n",
      "Steps: 846 Loss: 0.1199246495962143, Acc: 0.9801980257034302\n",
      "Steps: 847 Loss: 0.11979036033153534, Acc: 0.9801980257034302\n",
      "Steps: 848 Loss: 0.11965638399124146, Acc: 0.9801980257034302\n",
      "Steps: 849 Loss: 0.11952266842126846, Acc: 0.9801980257034302\n",
      "Steps: 850 Loss: 0.11938924342393875, Acc: 0.9801980257034302\n",
      "Steps: 851 Loss: 0.11925608664751053, Acc: 0.9801980257034302\n",
      "Steps: 852 Loss: 0.11912321299314499, Acc: 0.9801980257034302\n",
      "Steps: 853 Loss: 0.11899064481258392, Acc: 0.9801980257034302\n",
      "Steps: 854 Loss: 0.11885831505060196, Acc: 0.9801980257034302\n",
      "Steps: 855 Loss: 0.11872631311416626, Acc: 0.9801980257034302\n",
      "Steps: 856 Loss: 0.11859456449747086, Acc: 0.9801980257034302\n",
      "Steps: 857 Loss: 0.11846309155225754, Acc: 0.9801980257034302\n",
      "Steps: 858 Loss: 0.11833193898200989, Acc: 0.9801980257034302\n",
      "Steps: 859 Loss: 0.11820103228092194, Acc: 0.9801980257034302\n",
      "Steps: 860 Loss: 0.11807039380073547, Acc: 0.9801980257034302\n",
      "Steps: 861 Loss: 0.1179400235414505, Acc: 0.9801980257034302\n",
      "Steps: 862 Loss: 0.11780992895364761, Acc: 0.9801980257034302\n",
      "Steps: 863 Loss: 0.117680124938488, Acc: 0.9801980257034302\n",
      "Steps: 864 Loss: 0.1175505742430687, Acc: 0.9801980257034302\n",
      "Steps: 865 Loss: 0.11742129176855087, Acc: 0.9801980257034302\n",
      "Steps: 866 Loss: 0.11729232221841812, Acc: 0.9801980257034302\n",
      "Steps: 867 Loss: 0.11716358363628387, Acc: 0.9801980257034302\n",
      "Steps: 868 Loss: 0.11703512072563171, Acc: 0.9801980257034302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 869 Loss: 0.11690691858530045, Acc: 0.9801980257034302\n",
      "Steps: 870 Loss: 0.11677898466587067, Acc: 0.9801980257034302\n",
      "Steps: 871 Loss: 0.11665135622024536, Acc: 0.9801980257034302\n",
      "Steps: 872 Loss: 0.11652398109436035, Acc: 0.9801980257034302\n",
      "Steps: 873 Loss: 0.11639682948589325, Acc: 0.9801980257034302\n",
      "Steps: 874 Loss: 0.11626998335123062, Acc: 0.9801980257034302\n",
      "Steps: 875 Loss: 0.1161433607339859, Acc: 0.9801980257034302\n",
      "Steps: 876 Loss: 0.11601699888706207, Acc: 0.9801980257034302\n",
      "Steps: 877 Loss: 0.11589092761278152, Acc: 0.9801980257034302\n",
      "Steps: 878 Loss: 0.11576510965824127, Acc: 0.9801980257034302\n",
      "Steps: 879 Loss: 0.1156395748257637, Acc: 0.9801980257034302\n",
      "Steps: 880 Loss: 0.11551428586244583, Acc: 0.9801980257034302\n",
      "Steps: 881 Loss: 0.11538926512002945, Acc: 0.9801980257034302\n",
      "Steps: 882 Loss: 0.11526450514793396, Acc: 0.9801980257034302\n",
      "Steps: 883 Loss: 0.11513996869325638, Acc: 0.9801980257034302\n",
      "Steps: 884 Loss: 0.11501570791006088, Acc: 0.9801980257034302\n",
      "Steps: 885 Loss: 0.11489173024892807, Acc: 0.9801980257034302\n",
      "Steps: 886 Loss: 0.11476796120405197, Acc: 0.9801980257034302\n",
      "Steps: 887 Loss: 0.11464444547891617, Acc: 0.9801980257034302\n",
      "Steps: 888 Loss: 0.11452121287584305, Acc: 0.9801980257034302\n",
      "Steps: 889 Loss: 0.11439821869134903, Acc: 0.9801980257034302\n",
      "Steps: 890 Loss: 0.1142754778265953, Acc: 0.9801980257034302\n",
      "Steps: 891 Loss: 0.11415302008390427, Acc: 0.9900990128517151\n",
      "Steps: 892 Loss: 0.11403077095746994, Acc: 0.9900990128517151\n",
      "Steps: 893 Loss: 0.1139087975025177, Acc: 0.9900990128517151\n",
      "Steps: 894 Loss: 0.11378706246614456, Acc: 0.9900990128517151\n",
      "Steps: 895 Loss: 0.11366555839776993, Acc: 0.9900990128517151\n",
      "Steps: 896 Loss: 0.11354434490203857, Acc: 0.9900990128517151\n",
      "Steps: 897 Loss: 0.11342334002256393, Acc: 0.9900990128517151\n",
      "Steps: 898 Loss: 0.11330262571573257, Acc: 0.9900990128517151\n",
      "Steps: 899 Loss: 0.11318214237689972, Acc: 0.9900990128517151\n",
      "Steps: 900 Loss: 0.11306186765432358, Acc: 0.9900990128517151\n",
      "Steps: 901 Loss: 0.11294187605381012, Acc: 0.9900990128517151\n",
      "Steps: 902 Loss: 0.11282210052013397, Acc: 0.9900990128517151\n",
      "Steps: 903 Loss: 0.11270257085561752, Acc: 0.9900990128517151\n",
      "Steps: 904 Loss: 0.11258330196142197, Acc: 0.9900990128517151\n",
      "Steps: 905 Loss: 0.11246427893638611, Acc: 0.9900990128517151\n",
      "Steps: 906 Loss: 0.11234549432992935, Acc: 0.9900990128517151\n",
      "Steps: 907 Loss: 0.1122269555926323, Acc: 0.9900990128517151\n",
      "Steps: 908 Loss: 0.11210862547159195, Acc: 0.9900990128517151\n",
      "Steps: 909 Loss: 0.11199057847261429, Acc: 0.9900990128517151\n",
      "Steps: 910 Loss: 0.11187271773815155, Acc: 0.9900990128517151\n",
      "Steps: 911 Loss: 0.1117551326751709, Acc: 0.9900990128517151\n",
      "Steps: 912 Loss: 0.11163774132728577, Acc: 0.9900990128517151\n",
      "Steps: 913 Loss: 0.11152062565088272, Acc: 0.9900990128517151\n",
      "Steps: 914 Loss: 0.11140375584363937, Acc: 0.9900990128517151\n",
      "Steps: 915 Loss: 0.11128710955381393, Acc: 0.9900990128517151\n",
      "Steps: 916 Loss: 0.1111706867814064, Acc: 0.9900990128517151\n",
      "Steps: 917 Loss: 0.11105453222990036, Acc: 0.9900990128517151\n",
      "Steps: 918 Loss: 0.11093857139348984, Acc: 0.9900990128517151\n",
      "Steps: 919 Loss: 0.11082285642623901, Acc: 0.9900990128517151\n",
      "Steps: 920 Loss: 0.11070738732814789, Acc: 0.9900990128517151\n",
      "Steps: 921 Loss: 0.11059213429689407, Acc: 0.9900990128517151\n",
      "Steps: 922 Loss: 0.11047708988189697, Acc: 0.9900990128517151\n",
      "Steps: 923 Loss: 0.11036230623722076, Acc: 0.9900990128517151\n",
      "Steps: 924 Loss: 0.11024775356054306, Acc: 0.9900990128517151\n",
      "Steps: 925 Loss: 0.11013341695070267, Acc: 0.9900990128517151\n",
      "Steps: 926 Loss: 0.11001930385828018, Acc: 0.9900990128517151\n",
      "Steps: 927 Loss: 0.1099054291844368, Acc: 0.9900990128517151\n",
      "Steps: 928 Loss: 0.10979174822568893, Acc: 0.9900990128517151\n",
      "Steps: 929 Loss: 0.10967837274074554, Acc: 0.9900990128517151\n",
      "Steps: 930 Loss: 0.10956516116857529, Acc: 0.9900990128517151\n",
      "Steps: 931 Loss: 0.10945216566324234, Acc: 0.9900990128517151\n",
      "Steps: 932 Loss: 0.10933942347764969, Acc: 0.9900990128517151\n",
      "Steps: 933 Loss: 0.10922689735889435, Acc: 0.9900990128517151\n",
      "Steps: 934 Loss: 0.10911460220813751, Acc: 0.9900990128517151\n",
      "Steps: 935 Loss: 0.1090025082230568, Acc: 0.9900990128517151\n",
      "Steps: 936 Loss: 0.10889063030481339, Acc: 0.9900990128517151\n",
      "Steps: 937 Loss: 0.10877902060747147, Acc: 0.9900990128517151\n",
      "Steps: 938 Loss: 0.10866761952638626, Acc: 0.9900990128517151\n",
      "Steps: 939 Loss: 0.10855641961097717, Acc: 0.9900990128517151\n",
      "Steps: 940 Loss: 0.1084454283118248, Acc: 0.9900990128517151\n",
      "Steps: 941 Loss: 0.10833467543125153, Acc: 0.9900990128517151\n",
      "Steps: 942 Loss: 0.10822412371635437, Acc: 0.9900990128517151\n",
      "Steps: 943 Loss: 0.10811377316713333, Acc: 0.9900990128517151\n",
      "Steps: 944 Loss: 0.10800369828939438, Acc: 0.9900990128517151\n",
      "Steps: 945 Loss: 0.10789382457733154, Acc: 0.9900990128517151\n",
      "Steps: 946 Loss: 0.10778413712978363, Acc: 0.9900990128517151\n",
      "Steps: 947 Loss: 0.10767467319965363, Acc: 0.9900990128517151\n",
      "Steps: 948 Loss: 0.10756539553403854, Acc: 0.9900990128517151\n",
      "Steps: 949 Loss: 0.10745641589164734, Acc: 0.9900990128517151\n",
      "Steps: 950 Loss: 0.10734760761260986, Acc: 0.9900990128517151\n",
      "Steps: 951 Loss: 0.10723898559808731, Acc: 0.9900990128517151\n",
      "Steps: 952 Loss: 0.10713058710098267, Acc: 0.9900990128517151\n",
      "Steps: 953 Loss: 0.10702240467071533, Acc: 0.9900990128517151\n",
      "Steps: 954 Loss: 0.1069144606590271, Acc: 0.9900990128517151\n",
      "Steps: 955 Loss: 0.1068066731095314, Acc: 0.9900990128517151\n",
      "Steps: 956 Loss: 0.1066991314291954, Acc: 0.9900990128517151\n",
      "Steps: 957 Loss: 0.10659178346395493, Acc: 0.9900990128517151\n",
      "Steps: 958 Loss: 0.10648467391729355, Acc: 0.9900990128517151\n",
      "Steps: 959 Loss: 0.1063777357339859, Acc: 0.9900990128517151\n",
      "Steps: 960 Loss: 0.10627104341983795, Acc: 0.9900990128517151\n",
      "Steps: 961 Loss: 0.10616451501846313, Acc: 0.9900990128517151\n",
      "Steps: 962 Loss: 0.10605821758508682, Acc: 0.9900990128517151\n",
      "Steps: 963 Loss: 0.10595212876796722, Acc: 0.9900990128517151\n",
      "Steps: 964 Loss: 0.10584625601768494, Acc: 0.9900990128517151\n",
      "Steps: 965 Loss: 0.10574055463075638, Acc: 0.9900990128517151\n",
      "Steps: 966 Loss: 0.10563506931066513, Acc: 0.9900990128517151\n",
      "Steps: 967 Loss: 0.10552980750799179, Acc: 0.9900990128517151\n",
      "Steps: 968 Loss: 0.10542476922273636, Acc: 0.9900990128517151\n",
      "Steps: 969 Loss: 0.10531987249851227, Acc: 0.9900990128517151\n",
      "Steps: 970 Loss: 0.1052151694893837, Acc: 0.9900990128517151\n",
      "Steps: 971 Loss: 0.10511075705289841, Acc: 0.9900990128517151\n",
      "Steps: 972 Loss: 0.10500647872686386, Acc: 0.9900990128517151\n",
      "Steps: 973 Loss: 0.10490243136882782, Acc: 0.9900990128517151\n",
      "Steps: 974 Loss: 0.1047985702753067, Acc: 0.9900990128517151\n",
      "Steps: 975 Loss: 0.1046949177980423, Acc: 0.9900990128517151\n",
      "Steps: 976 Loss: 0.10459139198064804, Acc: 0.9900990128517151\n",
      "Steps: 977 Loss: 0.10448817163705826, Acc: 0.9900990128517151\n",
      "Steps: 978 Loss: 0.10438510030508041, Acc: 0.9900990128517151\n",
      "Steps: 979 Loss: 0.10428223013877869, Acc: 0.9900990128517151\n",
      "Steps: 980 Loss: 0.10417956858873367, Acc: 0.9900990128517151\n",
      "Steps: 981 Loss: 0.10407709330320358, Acc: 0.9900990128517151\n",
      "Steps: 982 Loss: 0.10397480428218842, Acc: 0.9900990128517151\n",
      "Steps: 983 Loss: 0.10387272387742996, Acc: 0.9900990128517151\n",
      "Steps: 984 Loss: 0.10377084463834763, Acc: 0.9900990128517151\n",
      "Steps: 985 Loss: 0.10366912186145782, Acc: 0.9900990128517151\n",
      "Steps: 986 Loss: 0.10356762260198593, Acc: 0.9900990128517151\n",
      "Steps: 987 Loss: 0.10346629470586777, Acc: 0.9900990128517151\n",
      "Steps: 988 Loss: 0.1033652201294899, Acc: 0.9900990128517151\n",
      "Steps: 989 Loss: 0.10326425731182098, Acc: 0.9900990128517151\n",
      "Steps: 990 Loss: 0.10316353291273117, Acc: 0.9900990128517151\n",
      "Steps: 991 Loss: 0.10306298732757568, Acc: 0.9900990128517151\n",
      "Steps: 992 Loss: 0.1029626727104187, Acc: 0.9900990128517151\n",
      "Steps: 993 Loss: 0.10286249220371246, Acc: 0.9900990128517151\n",
      "Steps: 994 Loss: 0.10276252776384354, Acc: 0.9900990128517151\n",
      "Steps: 995 Loss: 0.10266273468732834, Acc: 0.9900990128517151\n",
      "Steps: 996 Loss: 0.10256316512823105, Acc: 0.9900990128517151\n",
      "Steps: 997 Loss: 0.1024637296795845, Acc: 0.9900990128517151\n",
      "Steps: 998 Loss: 0.10236454010009766, Acc: 0.9900990128517151\n",
      "Steps: 999 Loss: 0.10226549208164215, Acc: 0.9900990128517151\n",
      "Steps: 1000 Loss: 0.10216663777828217, Acc: 0.9900990128517151\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs = 1000, verbose = 100):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        \n",
    "        acc = prediction(X, Y).numpy()\n",
    "        loss = cost_fn(X, Y).numpy()\n",
    "        print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
